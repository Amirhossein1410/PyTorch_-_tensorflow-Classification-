{"cells":[{"cell_type":"code","execution_count":6,"metadata":{"id":"HRkMAzJ7PND3","executionInfo":{"status":"ok","timestamp":1700987365953,"user_tz":-210,"elapsed":400,"user":{"displayName":"Amir Hossein Hosseini","userId":"00054851132641383807"}}},"outputs":[],"source":["import os\n","os.chdir(\"/content/drive/MyDrive/Colab/tensorflow_&_pytorch_classifier\")"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1-WOZ-yoFYCRkr0bJomqDR9-h-T4-jSkZ"},"id":"Z8qXl-0nPUmd","outputId":"4a6dc4d9-5cfe-4c17-c8a3-7aa15018e21c","executionInfo":{"status":"ok","timestamp":1700986797774,"user_tz":-210,"elapsed":668012,"user":{"displayName":"Amir Hossein Hosseini","userId":"00054851132641383807"}}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["from plot_model import PlotModel\n","from customization import Customize , LR_ASK\n","from make_model import MakeModel\n","import os\n","import pandas as pd\n","import sys\n","if not sys.warnoptions:\n","    import warnings\n","    warnings.simplefilter(\"ignore\")\n","pd.set_option('display.max_columns', None)  # or 1000\n","pd.set_option('display.max_rows', None)  # or 1000\n","pd.set_option('display.max_colwidth', None)  # or 199\n","\n","def run():\n","    Customizing = Customize(framework = 'tensorflow')\n","    msg = 'enter the full path to the working directory where data will be stored.'\n","    working_dir=input(msg)\n","    if 'kaggle' in working_dir:  # if running the notebook on kaggle need to modify so parameters\n","        delimiter = '/'\n","    else:\n","        delimiter = '\\\\'\n","    msg = 'Enter the full path to the train directory'\n","    train_dir =input(msg)\n","    msg = 'Enter the full path to the validation directory. If there is no validation directory press enter'\n","    ans = input(msg)\n","    if ans == '':\n","      val_dir = None\n","    else:\n","        valid_dir = ans\n","    msg = 'Enter the full path to the test directory. If there is no test directory press enter'\n","    ans = input(msg)\n","    if ans == '':\n","      test_dir = None\n","    else:\n","      test_dir = ans\n","    ftotal, flargest, maxclass, fsmallest, minclass = Customizing.check_dataset_size(train_dir)  # get info on training data\n","    msg1 = f'the train directory contains {ftotal} files, class {maxclass} has the most images of {flargest} files\\n'\n","    msg2 = f'class {minclass} contains the least number of images of {fsmallest} files\\n'\n","    msg3 = f'NOTE if the value of most images is the same as the value of least images the dataset is balanced\\n'\n","    msg4 = 'When dealing with very large data sets to save time you may not want to read in all the image files\\n'\n","    msg5 = 'to limit the maximum number of image files in any class you can enter an integer limiter value'\n","    msg = msg1 + msg2 + msg3 + msg4 + msg5\n","    PlotModel.print_in_color(msg)\n","    msg = 'input a limiter integer value to limit max number of images in a class, for no limit hit enter'\n","    limiter = input(msg)\n","    if limiter == '':\n","      limiter = None\n","    else:\n","      limiter = int(limiter)\n","      msg = f'images will be limited to a maximum of {limiter} images in each class'\n","      PlotModel.print_in_color(msg)\n","    # create train, test and valid data frames\n","    train_df, test_df, valid_df, classes, class_count, max_samples, min_samples, have, wave = Customizing.make_data_frames(train_dir , test_dir , val_dir  , limiter )\n","    msg = f'the average height of training images is {have} and average width is {wave}\\n  enter the image height to be used to train the model'\n","    img_height = int(input(msg))\n","    msg = ' Enter the image width to be used to train the model'\n","    img_width = int(input(msg))\n","    img_size = (img_height, img_width)\n","    msg = f'model will be trained with image shape of  ( {img_height}, {img_width} )'\n","    PlotModel.print_in_color(msg)\n","    # determine if user wants to balance the training dataframe\n","    train_df, img_size, classes, class_count = Customizing.preprocess_dataset(train_df, img_size, max_samples, min_samples,\n","                                                                  working_dir)\n","    # make train, test and valid data generators\n","    train_gen, test_gen, valid_gen, test_steps, class_count = Customizing.make_gens(20, 'labels', train_df, test_df, valid_df,\n","                                                                        img_size)\n","    PlotModel.show_image_samples(train_gen)  # show some sample training images\n","    msg = 'select model size enter S for small, L for large or hit enter (recommended) for medium size model'\n","    ans = input(msg)\n","    model = MakeModel.make_model(img_size, class_count, .001, ans)  # create the model\n","    epochs = 100\n","    ask_epoch = 10\n","    ask = LR_ASK(model, epochs=epochs, ask_epoch=ask_epoch)  # instantiate the custom callback\n","    callbacks = [ask]\n","    msg = f'Your model will start training. After {ask_epoch} epochs you will be asked if you wish to halt training by entering H\\nor enter the number of additional epochs to run, then you will be queired again after those epochs complete.\\n To decide look at the % decrease in validation loss if it is less than 2% enter H to halt training\\n Note your model is set with the weights from the epoch with lowest validation loss'\n","    PlotModel.print_in_color(msg)\n","    history = model.fit(x=train_gen, epochs=epochs, verbose=1, callbacks=callbacks, validation_data=valid_gen,\n","                        validation_steps=None, shuffle=True, initial_epoch=0)  # train the model\n","    PlotModel.tr_plot(history)  # plot training data\n","    msg = 'To save the training data to a csv file enter the name for the csv file or press enter to not save the data'\n","    ans = input(msg)\n","    if ans != '':  # same the training data to a csv file history.csv in working directory\n","        csvpath = os.path.join(working_dir, ans + '.csv')\n","        Customizing.save_history_to_csv(history, csvpath)\n","        msg = f'training data saved to {csvpath}'\n","        PlotModel.print_in_color(msg)\n","    # do predictions on test set and generate reports\n","    errors, tests, error_list, error_pred_list, f1score = Customizing.predictor(model,test_gen)\n","    Customizing.print_errors(error_list, error_pred_list, delimiter)\n","    msg = f'your trained model will be saved to directory {working_dir} enter a subject for the saved model'\n","    subject = input(msg)\n","    MakeModel.save_model(model, subject, classes, img_size, f1score, working_dir)  # save the model to the working directory\n","    msg = 'model save nomenclature is directory/subject-number of classes- (img_height, img_width)- F1score.h5'\n","    PlotModel.print_in_color(msg)\n","    msg = 'to run the classifier again enter R, to halt press enter  '\n","    ans = input(msg)\n","    if ans == 'R' or ans == 'r':\n","        run()\n","    else:\n","        PlotModel.print_in_color('Process Complete')\n","    return error_list, error_pred_list\n","\n","if __name__ == '__main__':\n","    error_list, error_pred_list = run()"]},{"cell_type":"code","execution_count":22,"metadata":{"id":"RkilbMjdPX81","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1700988966051,"user_tz":-210,"elapsed":8558,"user":{"displayName":"Amir Hossein Hosseini","userId":"00054851132641383807"}},"outputId":"6c29c44d-4dfc-404e-e90f-3d84ccc0cef6"},"outputs":[{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 3s 3s/step\n"]}],"source":["from tensorflow.keras.models import load_model\n","import numpy as np\n","\n","def F1_score(y_true, y_pred):  # taken from old keras source code\n","    from keras import backend as K\n","    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n","    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n","    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n","    precision = true_positives / (predicted_positives + K.epsilon())\n","    recall = true_positives / (possible_positives + K.epsilon())\n","    f1_val = 2 * (precision * recall) / (precision + recall + K.epsilon())\n","    return f1_val\n","\n","model= load_model(\"augment for fastfood/fast_food-6-(256 X 256)-98.03.h5\" , custom_objects={'F1_score' : F1_score})\n","model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy' , F1_score , 'AUC'])\n","\n","\n","from keras.preprocessing import image\n","imagePath = 'download (1).jpg'\n","test_image = image.load_img(imagePath, target_size = (256, 256))\n","test_image = image.img_to_array(test_image)\n","test_image = np.expand_dims(test_image, axis = 0)\n","\n","#predict the result\n","result = model.predict(test_image)"]},{"cell_type":"code","source":["result"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gOYmrZYPvWIP","executionInfo":{"status":"ok","timestamp":1700989492775,"user_tz":-210,"elapsed":8,"user":{"displayName":"Amir Hossein Hosseini","userId":"00054851132641383807"}},"outputId":"0c2cedd8-cdbc-4e3e-9d30-84f9bf85eb98"},"execution_count":27,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0.5943669 , 0.05219473, 0.04913924, 0.11753181, 0.15681055,\n","        0.02995676]], dtype=float32)"]},"metadata":{},"execution_count":27}]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"gpuType":"T4","mount_file_id":"1l_4pGToSEVYGr9IMOXcGhcU-aQCERres","authorship_tag":"ABX9TyMffnNYCMeGvnMCeMOEzSPC"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}